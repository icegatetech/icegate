# Ingest Binary Configuration Example
#
# Copy this file to config.ingest.toml and adjust the values for your environment.

# Catalog Configuration
[catalog]
# Catalog type: "memory", or struct variant with config
# For REST catalog:
backend = { rest = { uri = "http://127.0.0.1:19120/iceberg" } }
# For Memory catalog (testing):
# backend = "memory"

# Warehouse location (root path for data storage)
warehouse = "s3://warehouse"

# Additional catalog properties (optional)
[catalog.properties]
prefix = "main"
# Add any custom catalog properties here

# Storage Configuration
[storage]
# Storage backend: struct variant with config
# For S3 storage:
# backend = { s3 = { bucket = "warehouse", region = "us-east-1" } }
# For S3 with custom endpoint:
backend = { s3 = { bucket = "warehouse", region = "us-east-1", endpoint = "https://127.0.0.1:9100" } }
# For FileSystem storage:
# backend = { filesystem = { root_path = "/var/lib/icegate/data" } }
# For Memory storage (testing):
# backend = "memory"

# Storage properties (optional)
[storage.properties]

# Queue Configuration (WAL)
[queue.common]
# Base path for queue segments (e.g. "s3://bucket/queue" or "file:///var/data/queue")
base_path = "s3://queue"
# Capacity of the write channel (bounded for backpressure)
channel_capacity = 1024
# Maximum number of rows in one WAL parquet row group
max_row_group_size = 10000

[queue.write]
# Number of retry attempts for write operations
write_retries = 3
# Parquet compression codec: "none", "snappy", "gzip", "lzo", "brotli", "lz4", "zstd"
compression = "zstd"
# Flush by rows after this many row-groups-worth of accumulated rows
records_per_flush_multiplier = 1
# Maximum bytes to accumulate before flush
max_bytes_per_flush = 67108864
# Maximum time to wait before flush (milliseconds)
flush_interval_ms = 1000

# Shift Configuration
[shift.read]
# max_record_batches_per_task = 128
# max_input_bytes_per_task = 67108864

[shift.write]
# row_group_size = 10000
# max_file_size_mb = 100
# table_cache_ttl_secs = 60

[shift.timeouts]
# plan_base_ms = 60000
# shift_base_ms = 120000
# shift_per_record_batch_ms = 200
# shift_per_segment_ms = 300
# commit_base_ms = 30000
# commit_per_parquet_file_ms = 100

[shift.jobsmanager]
# worker_count = 1
# poll_interval_ms = 1000
# iteration_interval_millisecs = 300

# Job storage for shift operations
[shift.jobsmanager.storage]
endpoint = "http://127.0.0.1:9000"
bucket = "jobs"
prefix = "shifter"
region = "us-east-1"
use_ssl = false
job_state_codec = "json"
request_timeout_secs = 5

# OTLP HTTP Server
[otlp_http]
enabled = true
host = "127.0.0.1"
port = 4318

# OTLP gRPC Server
[otlp_grpc]
enabled = true
host = "127.0.0.1"
port = 4317
